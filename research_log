May 21 
What's done:
run_cz_hmm_mle.r 
Proves that with a small group of people, the mle works well to cluster 
The trick is large population + few cliques 

Need to do: 
1, R HMM package is shit 
2, Python BW algorithm works better than R HMM
3, Design simulation test for both MLE and BW 
	MLE test: 	
	 	1, number of chains 
	 	2, soft_diff 
	 	3, num_ind / K 

	BW test: 
		1, length of seq 
		2, init A and B 
		3, num_ind / K 
		4, debug viterbi 

		
May 23

Sampling ideas 
	1, sample integer partitions of total number of student N, with K cliques + brute force N choose summand_1, N - summand_1 choose summand_2, …. 

—> pre-run all the partitions 
-> each time, sample a partition, and then choose student according to the partition 

May 26 

Done: 
  Implemented the sampling ideas. Allow the algorithm to make sudden huge jumps --> see sample_partition.r to see details 
And committed 

Need to do: 
  Design simulation: for the same eps and lambda 
    1, num_ind = [12, 40, 60]
    2, K = [3, 5, 8, 10]
    3, num_chains [1, 2, 3, 4, 5]
    4, soft_diff [0.1, 0.3, 0.5]

Finished writing the actual scripts: run_r_hmm_mle.py, run_cz_hmm_mle_shell.r, run_cz_hmm_mle.r to automate the simulation design. 

    
     
    
